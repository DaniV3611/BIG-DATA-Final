# Parcial Final - Big Data Pipeline

Este proyecto implementa un pipeline completo de procesamiento de datos de noticias utilizando servicios de AWS, incluyendo extracciÃ³n web, procesamiento, almacenamiento y anÃ¡lisis con machine learning.

## DescripciÃ³n del Proyecto

Pipeline de datos que extrae, procesa y analiza noticias de periÃ³dicos colombianos utilizando arquitectura serverless y servicios de AWS.

## Progreso del Proyecto

### âœ… Completados

#### a) Lambda de ExtracciÃ³n Web con Zappa âœ…

Crear un lambda usando **Zappa** que descargue cada dÃ­a la pÃ¡gina principal de:

- El Tiempo
- El Espectador (o PublÃ­metro)

**Estructura de almacenamiento en S3:**

```
s3://bucket/headlines/raw/contenido-yyyy-mm-dd.html
```

#### b) Lambda de Procesamiento con BeautifulSoup âœ…

Una vez llega el archivo a la carpeta `raw`, se debe activar un segundo lambda que procese los datos utilizando **BeautifulSoup**.

**ExtracciÃ³n de datos:**

- CategorÃ­a
- Titular
- Enlace

**Estructura de salida CSV:**

```
s3://bucket/headlines/final/periodico=xxx/year=xxx/month=xxx/day=xxx
```

#### c) Lambda de ActualizaciÃ³n de CatÃ¡logo âœ…

Crear un tercer lambda que ejecute un **crawler en Glue** (usando boto3) para:

- Actualizar las particiones en el catÃ¡logo de Glue
- Permitir visualizaciÃ³n de datos por **AWS Athena**

#### d) MigraciÃ³n a Glue Jobs y Workflows âœ…

Repetir los puntos **a)** al **c)** implementados como:

- **Jobs de Python en Glue** âœ…
- Articulados en un **workflow** como el del parcial 2 âœ…

**ðŸ“‚ ImplementaciÃ³n disponible en:** `glue_jobs/`

**CaracterÃ­sticas implementadas:**
- 3 Glue Jobs (extractor, processor, crawler)
- Workflow completo con triggers condicionales
- Script de deployment automatizado
- Suite de testing comprehensiva
- DocumentaciÃ³n detallada

#### f) Pipeline de Machine Learning con PySpark âœ…

Crear un pipeline de procesamiento usando **PySpark ML** en **Notebook sobre EMR**:

**CaracterÃ­sticas implementadas:**

- âœ… VectorizaciÃ³n con **TF-IDF**
- âœ… Modelo de clasificaciÃ³n **Logistic Regression**
- âœ… Script ejecutable para EMR con **spark-submit**
- âœ… Resultados escritos en **S3** (mÃºltiples formatos)
- âœ… Manejo robusto de errores y logging
- âœ… EvaluaciÃ³n train/test del modelo

**ðŸ“‚ ImplementaciÃ³n disponible en:** `emr_scripts/`

#### g) AutomatizaciÃ³n EMR con Lambda âœ…

**ImplementaciÃ³n completa:**

- âœ… Convertir notebook anterior en **script ejecutable**
- âœ… Crear lambda que:
  - Lance un cluster EMR automÃ¡ticamente
  - Ejecute el script con `spark-submit`
  - Monitoree la ejecuciÃ³n completa
  - Apague el cluster automÃ¡ticamente (ahorro de costos)

**ðŸ“‚ ImplementaciÃ³n disponible en:** `lambdas/emr_manager/`

**CaracterÃ­sticas del Lambda:**
- ConfiguraciÃ³n automÃ¡tica de cluster EMR
- Upload dinÃ¡mico del script a S3
- Monitoreo en tiempo real de la ejecuciÃ³n
- Cleanup automÃ¡tico de recursos
- Manejo robusto de errores y timeouts
- Soporte para configuraciÃ³n personalizada

### ðŸš§ Pendientes

#### e) IntegraciÃ³n con RDS MySQL

**Base de datos:**

- Crear BD **MySQL en RDS** con la tabla respectiva
- Mapear con un crawler al catÃ¡logo de Glue

**Job de inserciÃ³n:**

- Usar **AWS Glue Connectors** y **AWS Job**
- Copiar de tabla a tabla (S3 â†’ RDS en el catÃ¡logo)
- **Activar "job bookmarks"** para evitar duplicados

## Requisitos de Entrega

### ðŸ“‹ Obligatorios

- **CÃ³digo en GitHub** con:
  - âœ… Uso de ramas
  - âœ… Commits descriptivos
  - âœ… CÃ³digo limpio y comentado
  - âœ… Pruebas unitarias (donde aplique)

> **âš ï¸ PenalizaciÃ³n:** Menos una unidad si no se cumple

### ðŸš€ Puntos Adicionales

- **Pipeline de despliegue continuo** en GitHub para scripts de jobs
- **Puntos a-d obligatorios** implementados por cÃ³digo con:
  - Pruebas unitarias
  - Despliegue continuo en GitHub

## TecnologÃ­as Utilizadas

- **AWS Lambda** - Funciones serverless
- **Zappa** - Framework para deployment de Lambda
- **AWS S3** - Almacenamiento de objetos
- **AWS Glue** - ETL y catÃ¡logo de datos
- **AWS Athena** - Consultas SQL sobre S3
- **AWS RDS MySQL** - Base de datos relacional
- **AWS EMR** - Cluster de Spark
- **BeautifulSoup** - Web scraping
- **PySpark ML** - Machine Learning distribuido
- **GitHub Actions** - CI/CD

## Estructura del Proyecto

```
â”œâ”€â”€ lambdas/
â”‚   â”œâ”€â”€ extractor/               # Lambda de extracciÃ³n web
â”‚   â”œâ”€â”€ processor/               # Lambda de procesamiento HTML
â”‚   â”œâ”€â”€ crawler/                # Lambda de crawler Glue
â”‚   â””â”€â”€ emr_manager/            # âœ… Lambda de gestiÃ³n EMR
â”‚       â”œâ”€â”€ lambda_function.py  # LÃ³gica principal del Lambda
â”‚       â”œâ”€â”€ requirements.txt    # Dependencias AWS
â”‚       â”œâ”€â”€ zappa_settings.json # ConfiguraciÃ³n deployment
â”‚       â””â”€â”€ README.md          # DocumentaciÃ³n completa
â”œâ”€â”€ glue_jobs/                  # âœ… Jobs y Workflows de Glue
â”‚   â”œâ”€â”€ extractor_job.py        # Job de extracciÃ³n migrado
â”‚   â”œâ”€â”€ processor_job.py        # Job de procesamiento migrado
â”‚   â”œâ”€â”€ crawler_job.py          # Job de crawler migrado
â”‚   â”œâ”€â”€ workflow_definition.py  # DefiniciÃ³n del workflow
â”‚   â”œâ”€â”€ deploy.py               # Script de deployment
â”‚   â”œâ”€â”€ test_jobs.py           # Suite de testing
â”‚   â”œâ”€â”€ requirements.txt        # Dependencias
â”‚   â””â”€â”€ README.md              # DocumentaciÃ³n detallada
â”œâ”€â”€ emr_scripts/               # âœ… Scripts para EMR
â”‚   â”œâ”€â”€ classification_pipeline.py  # Script de ML ejecutable
â”‚   â”œâ”€â”€ requirements.txt        # Dependencias de EMR
â”‚   â””â”€â”€ README.md              # DocumentaciÃ³n de EMR
â”œâ”€â”€ notebook/                  # ðŸ““ Notebooks de desarrollo
â”‚   â””â”€â”€ classification.ipynb   # Notebook original de ML
â”œâ”€â”€ tests/                     # Pruebas unitarias
â”œâ”€â”€ .github/workflows/         # CI/CD pipelines
â””â”€â”€ README.md                  # Esta documentaciÃ³n
```

## ðŸš€ Quick Start - Glue Jobs (Punto d)

### 1. Configurar credenciales AWS
```bash
aws configure
```

### 2. Desplegar Glue Jobs y Workflow
```bash
cd glue_jobs/
python deploy.py YOUR_BUCKET_NAME YOUR_IAM_ROLE_ARN us-east-1
```

### 3. Probar el workflow
```bash
python test_jobs.py all YOUR_BUCKET_NAME
```

### 4. Verificar en AWS Console
- **AWS Glue > Workflows**: Verificar `news-processing-workflow`
- **AWS Athena**: Consultar datos en `news_headlines_db`
- **S3**: Verificar estructura de particiones

Para mÃ¡s detalles, consultar: [`glue_jobs/README.md`](glue_jobs/README.md)

## ðŸš€ Quick Start - EMR ML Pipeline (Punto f)

### 1. Subir script a S3
```bash
aws s3 cp emr_scripts/classification_pipeline.py s3://your-bucket/scripts/
```

### 2. Ejecutar en cluster EMR
```bash
spark-submit \
    --deploy-mode cluster \
    --driver-memory 4g \
    --executor-memory 4g \
    s3://your-bucket/scripts/classification_pipeline.py \
    --input-path "s3://final-gizmo/headlines/final/periodico=*/year=*/month=*/day=*/*.csv" \
    --output-path "s3://final-gizmo/resultados/"
```

Para mÃ¡s detalles, consultar: [`emr_scripts/README.md`](emr_scripts/README.md)

## ðŸš€ Quick Start - EMR Lambda Manager (Punto g)

### 1. Desplegar Lambda con Zappa
```bash
cd lambdas/emr_manager/
pip install -r requirements.txt
pip install zappa
zappa deploy dev
```

### 2. Ejecutar pipeline completo
```bash
# EjecuciÃ³n bÃ¡sica
aws lambda invoke \
    --function-name emr-manager-dev \
    --payload '{}' \
    response.json

# Con configuraciÃ³n personalizada
aws lambda invoke \
    --function-name emr-manager-dev \
    --payload '{"core_instance_count": 3, "timeout_minutes": 90}' \
    response.json
```

### 3. Programar ejecuciÃ³n diaria
```bash
aws events put-rule \
    --name "daily-ml-pipeline" \
    --schedule-expression "cron(0 2 * * ? *)"
```

Para mÃ¡s detalles, consultar: [`lambdas/emr_manager/README.md`](lambdas/emr_manager/README.md)

## ðŸ“ˆ Roadmap

- [x] **Punto a)** - Lambda Extractor con Zappa
- [x] **Punto b)** - Lambda Processor con BeautifulSoup  
- [x] **Punto c)** - Lambda Crawler para Glue
- [x] **Punto d)** - MigraciÃ³n a Glue Jobs y Workflows
- [ ] **Punto e)** - IntegraciÃ³n con RDS MySQL
- [x] **Punto f)** - Pipeline de ML con PySpark (Script âœ…)
- [x] **Punto g)** - AutomatizaciÃ³n EMR con Lambda (âœ… COMPLETADO)
- [ ] **CI/CD** - Pipeline de despliegue continuo
- [ ] **Testing** - Cobertura completa de pruebas

## ðŸ“Š Arquitectura Actual

```mermaid
graph TB
    subgraph "Glue Workflow (âœ… Implementado)"
        A[Daily Trigger<br/>6 AM UTC] --> B[Extractor Job]
        B --> C[Processor Job]  
        C --> D[Crawler Job]
    end
    
    subgraph "Storage & Catalog"
        E[S3 Raw HTML]
        F[S3 Partitioned CSV]
        G[Glue Data Catalog]
        H[Athena Queries]
    end
    
    B --> E
    C --> F
    D --> G
    G --> H
    
    subgraph "EMR ML Pipeline (âœ… Automatizado)"
        I[Lambda EMR Manager]
        J[EMR Cluster Auto]
        K[Classification Script]
        L[ML Results S3]
    end
    
    I --> J
    J --> K
    K --> L
    F --> J
    
    subgraph "Triggers & Automation"
        M[Manual Trigger]
        N[Scheduled Trigger]
        O[API Gateway]
    end
    
    M --> I
    N --> I
    O --> I
    
    subgraph "Future (Pendiente)"
        P[RDS MySQL]
    end
    
    F -.-> P
```

## ðŸ”— Enlaces Ãštiles

- [DocumentaciÃ³n AWS Glue](https://docs.aws.amazon.com/glue/)
- [DocumentaciÃ³n AWS EMR](https://docs.aws.amazon.com/emr/)
- [DocumentaciÃ³n AWS Lambda](https://docs.aws.amazon.com/lambda/)
- [DocumentaciÃ³n Zappa](https://github.com/zappa/Zappa)
- [AWS CLI Setup](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html)
- [BeautifulSoup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [PySpark ML Guide](https://spark.apache.org/docs/latest/ml-guide.html)
