# Parcial Final - Big Data Pipeline

Este proyecto implementa un pipeline completo de procesamiento de datos de noticias utilizando servicios de AWS, incluyendo extracciÃ³n web, procesamiento, almacenamiento y anÃ¡lisis con machine learning.

## DescripciÃ³n del Proyecto

Pipeline de datos que extrae, procesa y analiza noticias de periÃ³dicos colombianos utilizando arquitectura serverless y servicios de AWS.

## Requisitos del Proyecto

### a) Lambda de ExtracciÃ³n Web con Zappa

Crear un lambda usando **Zappa** que descargue cada dÃ­a la pÃ¡gina principal de:

- El Tiempo
- El Espectador (o PublÃ­metro)

**Estructura de almacenamiento en S3:**

```
s3://bucket/headlines/raw/contenido-yyyy-mm-dd.html
```

### b) Lambda de Procesamiento con BeautifulSoup

Una vez llega el archivo a la carpeta `raw`, se debe activar un segundo lambda que procese los datos utilizando **BeautifulSoup**.

**ExtracciÃ³n de datos:**

- CategorÃ­a
- Titular
- Enlace

**Estructura de salida CSV:**

```
s3://bucket/headlines/final/periodico=xxx/year=xxx/month=xxx/day=xxx
```

### c) Lambda de ActualizaciÃ³n de CatÃ¡logo

Crear un tercer lambda que ejecute un **crawler en Glue** (usando boto3) para:

- Actualizar las particiones en el catÃ¡logo de Glue
- Permitir visualizaciÃ³n de datos por **AWS Athena**

### d) MigraciÃ³n a Glue Jobs y Workflows

Repetir los puntos **a)** al **c)** implementados como:

- **Jobs de Python en Glue**
- Articulados en un **workflow** como el del parcial 2

### e) IntegraciÃ³n con RDS MySQL

**Base de datos:**

- Crear BD **MySQL en RDS** con la tabla respectiva
- Mapear con un crawler al catÃ¡logo de Glue

**Job de inserciÃ³n:**

- Usar **AWS Glue Connectors** y **AWS Job**
- Copiar de tabla a tabla (S3 â†’ RDS en el catÃ¡logo)
- **Activar "job bookmarks"** para evitar duplicados

### f) Pipeline de Machine Learning con PySpark

Crear un pipeline de procesamiento usando **PySpark ML** en **Notebook sobre EMR**:

**CaracterÃ­sticas:**

- VectorizaciÃ³n con **TF-IDF**
- Modelo de clasificaciÃ³n (si aplica conocimiento de Aprendizaje de MÃ¡quina)
- Resultados escritos en **S3**

### g) AutomatizaciÃ³n EMR con Lambda

**ImplementaciÃ³n:**

- Convertir notebook anterior en **script ejecutable**
- Crear lambda que:
  - Lance un cluster EMR
  - Ejecute el script con `spark-submit`
  - Apague el cluster automÃ¡ticamente

## Requisitos de Entrega

### ğŸ“‹ Obligatorios

- **CÃ³digo en GitHub** con:
  - âœ… Uso de ramas
  - âœ… Commits descriptivos
  - âœ… CÃ³digo limpio y comentado
  - âœ… Pruebas unitarias (donde aplique)

> **âš ï¸ PenalizaciÃ³n:** Menos una unidad si no se cumple

### ğŸš€ Puntos Adicionales

- **Pipeline de despliegue continuo** en GitHub para scripts de jobs
- **Puntos a-d obligatorios** implementados por cÃ³digo con:
  - Pruebas unitarias
  - Despliegue continuo en GitHub

## TecnologÃ­as Utilizadas

- **AWS Lambda** - Funciones serverless
- **Zappa** - Framework para deployment de Lambda
- **AWS S3** - Almacenamiento de objetos
- **AWS Glue** - ETL y catÃ¡logo de datos
- **AWS Athena** - Consultas SQL sobre S3
- **AWS RDS MySQL** - Base de datos relacional
- **AWS EMR** - Cluster de Spark
- **BeautifulSoup** - Web scraping
- **PySpark ML** - Machine Learning distribuido
- **GitHub Actions** - CI/CD

## Estructura del Proyecto

```
â”œâ”€â”€ lambdas/
â”‚   â”œâ”€â”€ extractor/
â”‚   â”œâ”€â”€ processor/
â”‚   â””â”€â”€ crawler/
â”œâ”€â”€ glue_jobs/
â”œâ”€â”€ emr_scripts/
â”œâ”€â”€ tests/
â”œâ”€â”€ .github/workflows/
â””â”€â”€ README.md
```
