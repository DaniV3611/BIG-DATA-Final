# EMR Scripts - News Classification Pipeline

Este directorio contiene scripts para ejecutar el pipeline de machine learning en Amazon EMR.

##  Archivos

### `classification_pipeline.py`
Script principal que implementa el pipeline de clasificaci贸n de noticias usando PySpark ML.

**Caracter铆sticas:**
- Convierte el notebook Jupyter en script ejecutable
- Valores fijos predefinidos (como en el notebook original)
- Manejo robusto de errores y logging
- Optimizado para ejecuci贸n en cluster EMR
- M煤ltiples formatos de salida (CSV, Parquet)

##  Uso del Script

### Ejecuci贸n Local (para testing)
```bash
spark-submit classification_pipeline.py
```

### Ejecuci贸n en EMR Cluster
```bash
spark-submit \
    --deploy-mode cluster \
    --driver-memory 4g \
    --executor-memory 4g \
    --executor-cores 2 \
    --num-executors 3 \
    classification_pipeline.py
```

## 锔 Configuraci贸n Fija

El script usa valores predefinidos (id茅nticos al notebook original):

| Configuraci贸n | Valor |
|---------------|-------|
| **Input Path** | `s3://final-gizmo/headlines/final/periodico=*/year=*/month=*/day=*/*.csv` |
| **Output Path** | `s3://final-gizmo/resultados` |
| **TF-IDF Features** | `10000` |
| **Max Iterations** | `10` |
| **App Name** | `NewsClassificationPipeline` |

##  Pipeline de Procesamiento

1. **Carga de Datos**: Lee archivos CSV particionados desde S3
2. **Preprocesamiento**: 
   - Limpieza de texto (caracteres especiales, normalizaci贸n)
   - Filtrado de registros vac铆os
   - An谩lisis de distribuci贸n de categor铆as
3. **Feature Engineering**:
   - Tokenizaci贸n de texto
   - Remoci贸n de stop words
   - Vectorizaci贸n TF-IDF
4. **Entrenamiento**: Entrenamiento de Logistic Regression en todo el dataset
5. **Predicciones**: Generaci贸n de predicciones para todo el dataset
6. **Almacenamiento**: 
   - CSV con predicciones principales
   - CSV con probabilidades
   - Parquet completo

##  Resultados

El script genera exactamente la misma salida que el notebook original:

```
s3://final-gizmo/resultados/
 predicciones.csv           # Predicciones principales (como notebook)
 predicciones_prob.csv      # Predicciones con probabilidades
 predicciones_parquet/      # Dataset completo en Parquet
```

### Formato de Salida CSV
```csv
clean_text,label,prediction
"ubicacin secreta carros alta gama...",Unidad investigativa,3.0
"procuradura abre investigacin...",Unidad investigativa,3.0
```

### Formato de Salida CSV con Probabilidades
```csv
clean_text,label,prediction,probability_str
"ubicacin secreta carros alta gama...",Unidad investigativa,3.0,"[0.001, 0.002, ...]"
```

##  Configuraci贸n para EMR

### Configuraci贸n Recomendada del Cluster
```json
{
  "Name": "news-classification-cluster",
  "ReleaseLabel": "emr-6.15.0",
  "Applications": [
    {"Name": "Spark"},
    {"Name": "Hadoop"}
  ],
  "InstanceGroups": [
    {
      "Name": "Master",
      "InstanceRole": "MASTER",
      "InstanceType": "m5.xlarge",
      "InstanceCount": 1
    },
    {
      "Name": "Core",
      "InstanceRole": "CORE", 
      "InstanceType": "m5.xlarge",
      "InstanceCount": 2
    }
  ]
}
```

### Configuraciones Spark Recomendadas
- `spark.sql.adaptive.enabled=true`: Optimizaci贸n adaptativa de consultas
- `spark.sql.adaptive.coalescePartitions.enabled=true`: Optimizaci贸n de particiones
- `spark.serializer=org.apache.spark.serializer.KryoSerializer`: Serializaci贸n optimizada

##  Logging

El script utiliza logging est谩ndar de Python con nivel INFO. Los logs incluyen:
- Estad铆sticas de carga de datos
- Progreso del entrenamiento
- Informaci贸n de guardado de resultados
- Distribuci贸n de predicciones

##  Troubleshooting

### Errores Comunes

1. **OutOfMemoryError**: Aumentar `--driver-memory` y `--executor-memory`
2. **Timeout en S3**: Verificar permisos IAM del cluster
3. **Particiones peque帽as**: Ajustar `num-executors` y `executor-cores`

### Para Modificar Configuraci贸n

Si necesitas cambiar las rutas o par谩metros, edita estas constantes al inicio del script:

```python
INPUT_PATH = "s3://tu-bucket/ruta/entrada/*.csv"
OUTPUT_BASE_PATH = "s3://tu-bucket/resultados"
NUM_FEATURES = 10000
MAX_ITER = 10
```

##  Pr贸ximos Pasos

Este script est谩 listo para ser usado por el Lambda del punto g) que:
1. Lanzar谩 un cluster EMR
2. Ejecutar谩 este script con `spark-submit`
3. Monitorear谩 la ejecuci贸n
4. Apagar谩 el cluster autom谩ticamente 